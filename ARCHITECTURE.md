# Reddit Job Scraper - Architecture Documentation

## ğŸ—ï¸ Project Structure

```
redbot/
â”œâ”€â”€ main.py                     # Application entry point
â”œâ”€â”€ config.yaml                 # Configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ core/                       # Core business logic
â”‚   â”œâ”€â”€ __init__.py            # Exports: JobProcessor, get_job_processor
â”‚   â””â”€â”€ job_processor.py       # Orchestrates all parsers (130 lines)
â”‚
â”œâ”€â”€ sources/                    # Job sources (Reddit, Discord, etc.)
â”‚   â”œâ”€â”€ __init__.py            # Exports: RedditStream, DiscordBot
â”‚   â”œâ”€â”€ reddit.py              # Reddit job scraper (105 lines)
â”‚   â””â”€â”€ discord.py             # Discord bot integration (164 lines)
â”‚
â”œâ”€â”€ parsers/                    # Content parsers
â”‚   â”œâ”€â”€ __init__.py            # Exports all parsers
â”‚   â”œâ”€â”€ salary.py              # Salary detection (287 lines)
â”‚   â”œâ”€â”€ experience.py          # Experience level detection (140 lines)
â”‚   â”œâ”€â”€ sentiment.py           # Sentiment analysis (112 lines)
â”‚   â”œâ”€â”€ nlp.py                 # NLP extraction (154 lines)
â”‚   â””â”€â”€ data/                  # Parser constants
â”‚       â”œâ”€â”€ red_flags.py       # Red flag keywords (62 lines)
â”‚       â””â”€â”€ tech_stack.py      # Technology keywords (49 lines)
â”‚
â”œâ”€â”€ data/                       # Data layer
â”‚   â”œâ”€â”€ database.py            # Database facade (118 lines)
â”‚   â”œâ”€â”€ models/                # Data models
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Exports all models
â”‚   â”‚   â”œâ”€â”€ job.py             # JobPosting model (79 lines)
â”‚   â”‚   â”œâ”€â”€ user.py            # User models (71 lines)
â”‚   â”‚   â””â”€â”€ analytics.py       # Analytics models (43 lines)
â”‚   â”œâ”€â”€ repositories/          # Repository pattern
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Exports repositories
â”‚   â”‚   â”œâ”€â”€ base_repository.py # Base repository (32 lines)
â”‚   â”‚   â”œâ”€â”€ job_repository.py  # Job data access (124 lines)
â”‚   â”‚   â””â”€â”€ user_repository.py # User data access (171 lines)
â”‚   â””â”€â”€ migrations/            # SQL migrations
â”‚       â”œâ”€â”€ 001_initial_schema.sql
â”‚       â”œâ”€â”€ 002_user_profiles.sql
â”‚       â””â”€â”€ 003_analytics.sql
â”‚
â”œâ”€â”€ utils/                      # Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py              # Logging system (130 lines)
â”‚   â”œâ”€â”€ config.py              # Configuration loader (148 lines)
â”‚   â”œâ”€â”€ constants.py           # Environment constants
â”‚   â”œâ”€â”€ health.py              # Health monitoring (200 lines)
â”‚   â”œâ”€â”€ rate_limiter.py        # Rate limiting (150 lines)
â”‚   â””â”€â”€ retry.py               # Retry logic (180 lines)
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â””â”€â”€ migrate_db.py          # Database migration runner
â”‚
â””â”€â”€ tests/                      # Test suite
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_parsers.py        # Parser tests (16 passing)
```

## ğŸ“¦ Module Organization

### Core Principles

1. **Single Responsibility** - Each module has one clear purpose
2. **Separation of Concerns** - Business logic, data access, and parsing are separate
3. **Repository Pattern** - Clean data access layer
4. **Dependency Injection** - Loose coupling between components
5. **Clean Imports** - All packages export their public API via `__init__.py`

### Module Breakdown

#### Sources (`sources/`)
**Purpose**: Job sources and external integrations
- `reddit.py` - Scrapes Reddit with age filtering and job processing
- `discord.py` - Sends rich embeds to Discord with all parsed data

#### Core (`core/`)
**Purpose**: Business logic orchestration
- `job_processor.py` - Integrates all parsers, processes jobs end-to-end

#### Parsers (`parsers/`)
**Purpose**: Extract structured data from unstructured text
- `salary.py` - Detects and normalizes salaries (16 test cases âœ“)
- `experience.py` - Classifies experience levels (Junior/Mid/Senior/Lead)
- `sentiment.py` - Analyzes sentiment and detects 60+ red flags
- `nlp.py` - Extracts skills, location, and requirements
- `data/` - Parser constants (red flags, tech stack)

#### Data (`data/`)
**Purpose**: Data models and persistence
- `database.py` - Facade for all repositories
- `models/` - Data classes with serialization
- `repositories/` - Data access with repository pattern
- `migrations/` - SQL schema migrations

#### Utils (`utils/`)
**Purpose**: Cross-cutting concerns
- `logger.py` - Structured logging with rotation
- `config.py` - YAML configuration loader
- `health.py` - Health monitoring and alerts
- `rate_limiter.py` - API rate limiting
- `retry.py` - Exponential backoff retry logic

## ğŸ”„ Data Flow

```
Reddit API
    â†“
RedditStream (sources/reddit.py)
    â”œâ”€ Filters by keywords
    â”œâ”€ Applies age filter
    â””â”€ Creates JobPosting objects
    â†“
JobProcessor (core/job_processor.py)
    â”œâ”€ SalaryParser â†’ Extracts salary
    â”œâ”€ ExperienceParser â†’ Detects level
    â”œâ”€ SentimentAnalyzer â†’ Finds red flags
    â””â”€ NLPExtractor â†’ Extracts skills/location
    â†“
JobPosting (enriched with parsed data)
    â†“
DiscordBot (sources/discord.py)
    â”œâ”€ Creates rich embed
    â”œâ”€ Color codes by priority
    â””â”€ Sends to Discord channel
    â†“
Database (data/repositories/)
    â””â”€ Stores job posting
```

## ğŸ’¡ Design Patterns Used

### 1. Repository Pattern
**Location**: `data/repositories/`
**Purpose**: Abstracts data access, makes testing easy
```python
db = get_database()
job = db.jobs.get_by_url(url)
db.jobs.save(job)
```

### 2. Facade Pattern
**Location**: `data/database.py`
**Purpose**: Provides simple interface to complex subsystems
```python
db = Database()
db.jobs  # JobRepository
db.users # UserRepository
```

### 3. Strategy Pattern
**Location**: `parsers/`
**Purpose**: Interchangeable parsing algorithms
```python
processor.salary_parser.parse(text)
processor.experience_parser.parse(text)
```

### 4. Singleton Pattern
**Location**: `utils/logger.py`, `data/database.py`
**Purpose**: Single instance of shared resources
```python
logger = setup_logger()  # Single instance
db = get_database()      # Single instance
```

## ğŸ¯ Key Improvements Over Original

| Aspect | Before | After |
|--------|--------|-------|
| **Module Count** | 12 files | 30+ focused files |
| **Avg File Size** | 200-400 lines | 100-200 lines |
| **Parsers** | Monolithic | Modular with data separation |
| **Database** | Single 400+ line file | Repository pattern (4 files) |
| **Data** | Hardcoded in code | Separate data files |
| **Imports** | Messy, direct imports | Clean via `__init__.py` |
| **Naming** | `components/` | `sources/` (clearer) |
| **Logging** | Custom bprint | Professional loguru |
| **Tests** | 16 tests | 16 tests (maintained) |

## ğŸ“ˆ Code Metrics

- **Total Lines**: ~2,500 lines
- **Test Coverage**: Parsers fully tested
- **Avg Module Size**: 120 lines
- **Max Module Size**: 287 lines (salary.py - comprehensive)
- **Min Module Size**: 32 lines (base_repository.py)

## ğŸ”§ Maintenance Guidelines

### Adding a New Parser
1. Create `parsers/new_parser.py`
2. Add exports to `parsers/__init__.py`
3. Integrate in `core/job_processor.py`
4. Write tests in `tests/test_parsers.py`

### Adding a New Job Source
1. Create `sources/new_source.py`
2. Inherit from base class or follow RedditStream pattern
3. Add exports to `sources/__init__.py`
4. Update `main.py` to include new source

### Adding Database Tables
1. Create migration in `data/migrations/`
2. Update models in `data/models/`
3. Add repository methods in appropriate repository
4. Run `python scripts/migrate_db.py`

## ğŸš€ Running the Application

```bash
# Install dependencies
pip install -r requirements.txt

# Run migrations
python scripts/migrate_db.py

# Start the bot
python main.py
```

## ğŸ§ª Running Tests

```bash
# Run all tests
python tests/test_parsers.py

# With pytest (if installed)
pytest tests/ -v
```

---

**This architecture follows SOLID principles and clean code practices for maintainability and scalability.**
